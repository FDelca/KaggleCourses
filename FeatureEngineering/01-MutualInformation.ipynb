{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First encountering a new dataset can sometimes feel overwhelming. One might be presented with hundreds or thousands of features without even a description to go by. Where to even begin?\n",
    "\n",
    "A good first step might be to construct a **ranking** with a **feature utility metric** - a function that measures associations between a feature and a target. With that, one can choose a smaller set of the most useful features to develop initially and have more confidence that the time will be well spent.\n",
    "\n",
    "This metric can be the [mutual information](https://medium.com/swlh/a-deep-conceptual-guide-to-mutual-information-a5021031fad0)\n",
    "\n",
    "\n",
    "Lets look at how Mutual Information is contructed: (insert an image here)\n",
    "\n",
    "\n",
    "It uses something known as **Kullback-Leible divergence**, which is a measure of how one probability distribution is different from a second, reference probability distribution. \n",
    "\n",
    "- **KL divergence = 0** - indicates that the two distributions in question are identical;\n",
    "\n",
    "\n",
    "Notes:\n",
    "1. There are various ways to use KL divergence. When talking in terms of Bayesian inference we can think of it as a measure of the information gained by revising one's beliefs from the prior probability distribution (the amount of information lost when **Q** is used to approximate **P**). In machine learning we call this **information gain**, while coding theory it refers to the extra bits needs to code samples from P using a code optimized for Q;\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
