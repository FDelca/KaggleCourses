{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is used for categorical features. It is a method of encoding categories as numbers, like OHE or label encoding, with the difference that it also uses the target to create the encoding. This makes it what we call a **suprevised** feature engineering technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "autos = pd.read_csv(\"data/autos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple and effective version is to apply a group aggregation from Lesson 3, like the mean. Using the Automobiles dataset, this computes the average price of each vehicle's make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>price</th>\n",
       "      <th>make_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>13495</td>\n",
       "      <td>15498.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>16500</td>\n",
       "      <td>15498.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alfa-romero</td>\n",
       "      <td>16500</td>\n",
       "      <td>15498.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audi</td>\n",
       "      <td>13950</td>\n",
       "      <td>17859.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audi</td>\n",
       "      <td>17450</td>\n",
       "      <td>17859.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>audi</td>\n",
       "      <td>15250</td>\n",
       "      <td>17859.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>audi</td>\n",
       "      <td>17710</td>\n",
       "      <td>17859.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>audi</td>\n",
       "      <td>18920</td>\n",
       "      <td>17859.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>audi</td>\n",
       "      <td>23875</td>\n",
       "      <td>17859.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bmw</td>\n",
       "      <td>16430</td>\n",
       "      <td>26118.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          make  price  make_encoded\n",
       "0  alfa-romero  13495  15498.333333\n",
       "1  alfa-romero  16500  15498.333333\n",
       "2  alfa-romero  16500  15498.333333\n",
       "3         audi  13950  17859.166667\n",
       "4         audi  17450  17859.166667\n",
       "5         audi  15250  17859.166667\n",
       "6         audi  17710  17859.166667\n",
       "7         audi  18920  17859.166667\n",
       "8         audi  23875  17859.166667\n",
       "9          bmw  16430  26118.750000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autos[\"make_encoded\"] = autos.groupby(\"make\")[\"price\"].transform(\"mean\")\n",
    "\n",
    "autos[[\"make\", \"price\", \"make_encoded\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of target encoding is sometimes called a **mean encoding**. Applied to a **binary target**, it's also called **bin counting**. (Other names you might come across include: likelihood encoding, impact encoding, and leave-one-out encoding.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An encoding like this presents a couple of problems:\n",
    "1. There are unknown categories; \n",
    "    - It creates a special risk of overfitting, which means they need to be trained on an independent \"encoding\" split; When you join the encoding to future splits, pandas will fill the missing values for any categories not present in the encoding split. These missing values will need to be imputed somehow;\n",
    "\n",
    "\n",
    "2. Rare categories:\n",
    "    - When a category only occurs a few times in the dataset, any statistics calculated on its group are unlikely to be very accurate. In the Automobiles dataset, the mercurcy make only occurs once. The \"mean\" price we calculated is just the price of that vehicle, which might not be very representative of any Mercuries we might see in the future. Target encoding rare categories can make overfitting more likely. \n",
    "\n",
    "A **solution** to these problems is to add **smoothing**.\n",
    "\n",
    "`The idea is to blend the in-category average with the overall average. Rare categories get less weight on their category average, while missing categories just get the overall average.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "encoding = weight * in_category + (1 - weight) * overall\n",
    "```\n",
    "where weight is a value between 0 and 1 calculated from the category frequency.\n",
    "\n",
    "An easy way to determine the value for weight is to compute an m-estimate:\n",
    "\n",
    "```python\n",
    "weight = n / (n + m)\n",
    "```\n",
    "where n is the total number of times that category occurs in the data. The parameter m determines the \"smoothing factor\". Larger values of m put more weight on the overall estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/2022-06-14-18-28-48.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Automobiles dataset there are three cars with the make chevrolet. If you chose m=2.0, then the chevrolet category would be encoded with 60% of the average Chevrolet price plus 40% of the overall average price.\n",
    "\n",
    "```python\n",
    "chevrolet = 0.6 * 6000.00 + 0.4 * 13285.03\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When choosing a value for `m`, consider how noisy you expect the categories to be. Does the price of a vehicle vary great deal within each make? Would you need a lot of data to get good estimates? If so, it could be better to choose a larger value for `m`; if the average price for each make were relatively stable, a smaller value could be ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use cases for Target Encoding:**\n",
    "1. **High-cardinality features** - A feature with a large number of categories can be troublesome to encode: a ohe would generate too many features and alternatives, like label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target;\n",
    "2. **Domain-motivated features** - From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help a feature's true informativeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : MovieLens1M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems in acquiring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import warnings\n",
    "\n",
    "# plt.style.use(\"seaborn-whitegrid\")\n",
    "# plt.rc(\"figure\", autolayout=True)\n",
    "# plt.rc(\n",
    "#     \"axes\",\n",
    "#     labelweight=\"bold\",\n",
    "#     labelsize=\"large\",\n",
    "#     titleweight=\"bold\",\n",
    "#     titlesize=14,\n",
    "#     titlepad=10,\n",
    "# )\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# df = pd.read_csv(\"data/movies_1m.csv\")\n",
    "# df = df.astype(np.uint8, errors='ignore') # reduce memory footprint\n",
    "# print(\"Number of Unique Zipcodes: {}\".format(df[\"Zipcode\"].nunique()))\n",
    "\n",
    "# X = df.copy()\n",
    "# y = X.pop('Rating')\n",
    "\n",
    "# X_encode = X.sample(frac=0.25)\n",
    "# y_encode = y[X_encode.index]\n",
    "# X_pretrain = X.drop(X_encode.index)\n",
    "# y_train = y[X_pretrain.index]\n",
    "\n",
    "# from category_encoders import MEstimateEncoder\n",
    "\n",
    "# # Create the encoder instance. Choose m to control noise.\n",
    "# encoder = MEstimateEncoder(cols=[\"Zipcode\"], m=5.0)\n",
    "\n",
    "# # Fit the encoder on the encoding split.\n",
    "# encoder.fit(X_encode, y_encode)\n",
    "\n",
    "# # Encode the Zipcode column to create the final training data\n",
    "# X_train = encoder.transform(X_pretrain)\n",
    "\n",
    "# plt.figure(dpi=90)\n",
    "# ax = sns.distplot(y, kde=False, norm_hist=True)\n",
    "# ax = sns.kdeplot(X_train.Zipcode, color='r', ax=ax)\n",
    "# ax.set_xlabel(\"Rating\")\n",
    "# ax.legend(labels=['Zipcode', 'Rating']);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('feats')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8733063b9b3939d41b10709acfb92625eb79cad54022e84321cdbbb171f19604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
