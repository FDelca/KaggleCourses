{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First encountering a new dataset can sometimes feel overwhelming. One might be presented with hundreds or thousands of features without even a description to go by. Where to even begin?\n",
    "\n",
    "A good first step might be to construct a **ranking** with a **feature utility metric** - a function that measures associations between a feature and a target. With that, one can choose a smaller set of the most useful features to develop initially and have more confidence that the time will be well spent.\n",
    "\n",
    "This metric can be the [mutual information](https://medium.com/swlh/a-deep-conceptual-guide-to-mutual-information-a5021031fad0)\n",
    "\n",
    "\n",
    "Lets look at how Mutual Information is contructed:\n",
    "\n",
    "![](images/2022-06-09-11-09-16.png)\n",
    "\n",
    "It uses something known as **Kullback-Leible divergence**, which is a measure of how one probability distribution is different from a second, reference probability distribution. \n",
    "\n",
    "### KL-divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a closer look at **KL-divergence** is mathematically defined:\n",
    "\n",
    "![](images/2022-06-09-11-30-36.png)\n",
    "\n",
    "[**Properties:**](https://sam-black.medium.com/demystified-kullback-leibler-divergence-3971f956ef34)\n",
    "1. It is non symmetric. Sometimes, you'll hear KL divergence called distance metric. It's a simpler way of understanding, but if we want to keep it technically correct, KL divergence does not satisfy the triangle inequality, so it is not a true distance. `KL_Divergence(P||Q) != KL_Divergence(Q||P)`. If we want a symmetric form, say for clustering or some other interesting use of distances in a topological space - use [Jensen Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence), also known as **information radius**;\n",
    "2. It is a summation for discrete probability distributions and an integral for continous probability distributions;\n",
    "3. **It is always ≥ 0** .\n",
    "\n",
    "We can see that KL-divergence is the expectation of the logarithmic difference between the probabilities P and Q. The expectation just means we’re calculating the average value, hence the summation sign summing over all values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-06-09-11-15-44.png)\n",
    "\n",
    "- **KL divergence = 0** - indicates that the two distributions in question are identical; We can see in the example above that the distributions *diverge* from one another (a non-zero result)\n",
    "\n",
    "Think of P as representing the **true distribution** of the data ('reality') while **Q** is out theory, **model**, or approximation of P. The intuition behind KL-divergenceis that is looks at how different our model or reality is from reality. In many applications we are thus looking to **minimize** the KL-divergence between **P** and **Q** in order to find the distribution **Q** (model) that is closest to the distribution **P** (reality).\n",
    "\n",
    "```\n",
    "There are several ways to think about KL-divergence. When talking in terms of Bayensian inference we can think of it as a measure of the information gained by revising one's beliefs from the prior probability distribution (the amount of information lost when Q is used to approximate P). In machine learning we call this information gain, while in coding theory it refers to the extra bits needed to code samples from P using a code optimized for Q.\n",
    "```\n",
    "\n",
    "In Mutual Information is quite different, instead of using plain distributions it uses the joint and the product of marginals. This is valid since a joint distribution is itself a single distribution. In other words, we can look for the KL-divergence between a single joint distribution and a single product distribution to measure the dependence between 2 variables. Since we are looking for how different the joint is from the product of marginals, we already know that if the joint differs from the product of marginals there is some **dependence** between variables in question. \n",
    "\n",
    "To truly understand Mutual Information we need to grasp what **information** is. A hint towards doing this is the fact that KL-divergence is also named **relative entropy**. If entropy is being used in the formulation of Mutual Information then it must have something to do with information itself.\n",
    "\n",
    "### Entropy and Information\n",
    "\n",
    "\n",
    "When we observe something we are exposed to some sort of information, and a model is exploiting that information to explain/predict something in general.\n",
    "\n",
    "![](images/2022-06-09-12-13-18.png)\n",
    "\n",
    "In this image it is possible to grasp the path from an observation to the use of a model. Making an observation is done for the sake of being **surprised** (we are not observing the obvious rather we wish to notice something we have not seen before). \n",
    "- **Information can be though of as the amount of surprise contained in an observation, also called as surprisal**;\n",
    "- The **more** surprisal associated with a variable the **more** information that variable contains; This makes intuitive sense, if I find out that the details of my observation are things that I already knew then it is not useful to consider these details as information.\n",
    "- To quantify this relationship between **observation - surprisal - information** we use **uncertainty**:\n",
    "    - Uncertainty connects us to **probability** since probability counts things in terms of likely and unlikely outcomes. \n",
    "    - Probability gives us the mathematical framework to quantify what we observe.\n",
    "\n",
    "\n",
    "**But how does entropy relate all these concepts together?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets follow a coin flip example to be easier, to understand:\n",
    "\n",
    "![](images/2022-06-09-12-31-41.png)                    ![](images/2022-06-09-12-32-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the relationship between bias and surprisal. \n",
    "- The **more we bend the coin** - **less entropy** -> **less surprised** we will be of the outcome.\n",
    "- We can see that a fair coin toss must have the most surprise, whereas a biased coin toss has less surprise. Since there is no “favoritism” when the coin is fair we have no idea to which side the coin will land, but if we bias one side there will be less surprisal.\n",
    "- We can also say that observing the result of a **fair coin** toss provides the **maximum amount of information**. We had no idea of the outcome prior to flipping the fair coin, so we learned more from this outcome than we would from a biased coin, thus containing the most amount of **uncertainty**\n",
    "- The result of a fair coin toss is also the least predictable, since there is no increased likelihood of having one event over the other.\n",
    "- We can see that a prerequisite for being able to **make a prediction** is the **existence of nonuniform probabilities that underly the possible events**.\n",
    "\n",
    "Extra notes:\n",
    "1. **Non-uniform probabilities** decrease the surprise/information/uncertainty;\n",
    "2. **Smaller probabilities** attached to each possible outcome increases entropy;\n",
    "\n",
    "\n",
    "To understand the **2** point we can compare a rolled die to a tossed coin:\n",
    "\n",
    "![](images/2022-06-09-12-39-54.png)\n",
    "\n",
    "- A rolle die has higher entropy than a tossed coin since each outcome of a die toss has a smaller probability (p=1/6) than each outcome of a coin toss (p=1/2);\n",
    "- Remember that all probabilities must sum to 1 thus smaller probabilities attached to each outcome means more possible outcomes and thus more surprise upon learning the actual outcome;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition Behind Logarithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithms convert multiplicative quantities into additive ones. Entropy is simply the log of the number of ways a system can be arranged, converting an underlying multiplicative phenomenon (things combining) into an additive tool.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/2022-06-09-14-37-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIgure 19 shows the difference between exponential growth viewed on a linear scale and the same growth viewed in a logarithm scale. Whereas the linear scale shows explosive behaviour the logarithmic scale \"tames\" this into a simple straight line. (A common way to detect a exponential growth is to see if it forms a straight line on a logarithmic scale)\n",
    "\n",
    "Exponential growth is associated with multiplicative processes. Graphically, think of logarithm as *offloading the explosive growth from a multiplicative process onto the axes* such that the function itself no longer has to bear that growth. In algebraic interpretations, recall that the logarithm is the inverse function to exponentiation. This means the logarithm of a given number x is the exponent to which another fixed number (the base) must be raised to produce x. If we keep taking the logarithm of a massive number we will get back a smaller number (the exponent) that counts by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/2022-06-09-14-43-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 20 depicts the use of a logarithm as the representation of some combinatorial explosion in terms of simple linear growth. Just as 2 dice \"explode\" in the number of possible combinations, so too do the various configurations a system can take on. Logarithms are what make working with massive numbers (like th number of ways atoms can be arranged in a material) more manageable.\n",
    "\n",
    "- The use of logarithms takes into account the complexity of the problem. In other words, we do not toss away the combinatorial behaviour of systems when we simplify them with logarithms. \"They are brought along for thre ride\" despite our simplication of the problem;\n",
    "- Contrasting this with Pearson's correlation that also simplifies a system by depicting it as simple vectors pointing in similar directions - but does it so by ignoring the complexity of the problem;\n",
    "- The goal in science is not to reduce phenomena down to the simplest description, it is to reduce phenomena down to the simplest description that retains the core properties of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WrapUp of Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that MI uses KL-divergence, meaning it uses the discrepancy between a joint distribution and the product of marginals to measure information. Let's build our intuition around joint and marginal probabilities using a classic example:\n",
    "\n",
    "![](images/2022-06-09-16-57-55.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider A and B as discrete random variables associated with the outcomes of the draw from each urn. The probability of drawing a red marble from either is 2/3 (since there are 8 red marbles out of 12). \n",
    "\n",
    "The various probabilities depending on the outcome are shown below:\n",
    "\n",
    "![](images/2022-06-09-17-00-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The center cells in green are - **the joint probability**;\n",
    "- The red cells are the - **marginal probabilities**;\n",
    "\n",
    "If we were using machine learning to classify cats in images containing either cats or dogs we would be comparing a vector of predicted labels to a vector of actual labels in a test set. \n",
    "Both of these vectors have distributions, which are either similar to each other or not. Thus a pratical use of L-divergence would be to use it as a **loss function** in a machine learning algorithmm where internal model parameters are adjusted until the delta between predicted and actual labelling is minimized.\n",
    "\n",
    "To connect this to the concept of joint and marginal probabilities let's reframe the comparison between predicted and actual vectors in terms of the urn example.\n",
    "\n",
    "![](images/2022-06-09-17-12-14.png)\n",
    "\n",
    "Each predicted label is compared to an actual label, which is analogous to a single draw from 2 urns. The learning algorithm is \"observing\" the draws from the \"urns\" as 1 of 4 possibilitues. Just as the example above, we can use this to fill the probability table:\n",
    "\n",
    "![](images/2022-06-09-17-14-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we are talking about the **confusion matrix**. We can view the confusion matrix in terms of probability by realizing that the center rectangle are the joint probabilities, while the outer row/column are the marginal probabilities. Now, we can see where the 2 distribution of MI are coming from:\n",
    "\n",
    "![](images/2022-06-09-17-16-39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap up by looking at an alternative expression of MI, compared to what we saw previously:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/2022-06-09-17-18-55.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here MI is depicted in terms of the additive and subtractive relationships of various infromation measures associated with variables X and Y. H(X) and H(Y) are the marginal entropies, while H(X,Y) is the joint entropy of X and Y. This tell us that mutual information is a joint entropy subtracted off marginal entropies. Check the Venn diagram:\n",
    "\n",
    "![](images/2022-06-09-17-21-21.png)\n",
    "\n",
    "- The are contained by both circles is the **joint entropy H(X,Y)**. Joint entropy is a measure of the uncertainty associated with a set of variables. In other words, if entropy is calculated by sandwiching individual probabilities around a log, then it makes sense that joint entropy would be the same thing but with joint probabilities.\n",
    "\n",
    "- The **individual entropies** are the entropy we already know, we can think of them as the expected value of the \"self-information\" contained within the variable. In other words, each variable has some level of \"surprisal\" (recall earlier) contained within it that we have yet to access. Upon learning the outcome (of the event) we will gain access to this information.\n",
    "\n",
    "![](2022-06-09-17-32-23.png)\n",
    "\n",
    "- The **conditional entropies**, which are the parts of the circles that are not overlapping, are defined as follows: \n",
    "\n",
    "![](images/2022-06-09-17-35-42.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Causality is a central concept in our lives, underlying virtually every decision we make. The industry standard approach to uncovering causality is Pearson’s correlation. There are a number of severe limitations baked into Pearson’s, relating to its assumptions regarding how random variables might be related. Mutual Information digs much deeper into the notion of dependence by focusing directly on information itself. Further, by understanding information in terms of its “physicality” we can see how both thermodynamic and information-theoretic interpretations of entropy make any entropic measure of dependency, like MI, truly fundamental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps:\n",
    "1. Understand better how **bayesian learning processes** work;"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
